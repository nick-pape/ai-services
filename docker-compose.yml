version: "3.9"

services:
  local-ai:
    image: localai/localai:v2.24.2-aio-gpu-nvidia-cuda-12
    container_name: local-ai
    restart: always
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      DEBUG: "true"
      VERBOSE: "true"
      GGUF_USE_CUBLAS: "1"
      LLAMA_CUBLAS: "1"
      NVIDIA_VISIBLE_DEVICES: "all"
      LOCALAI_LOG_LEVEL: "debug"
      NVIDIA_DRIVER_CAPABILITIES: "compute,utility"
      LOCALAI_F16: "true"

      # ðŸš€ Extra environment variables to force CUDA on CLIP
      GGML_CUDA: "1"
      GGML_CUDA_FORCE_MMQ: "1"
      GGML_CUDA_FORCE_CUBLAS: "1"
      CLIP_CUBLAS: "1"
      CLIP_CUDA: "1"
      CLIP_USE_CUDA: "1"
      GGUF_CUBLAS: "1"

    volumes:
      - ./models:/build/models
      - /usr/lib/wsl/lib/nvidia-smi:/usr/local/bin/nvidia-smi
    ports:
      - "8081:8080"
    stdin_open: true
    tty: true

  kokoro-fastapi:
    image: ghcr.io/remsky/kokoro-fastapi-gpu:latest
    container_name: kokoro-fastapi
    restart: always
    environment:
      NVIDIA_REQUIRE_CUDA: "cuda>=12.6"
      CUDA_VERSION: "12.6.0"
      NVIDIA_VISIBLE_DEVICES: "all"
      NVIDIA_DRIVER_CAPABILITIES: "compute,utility"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    ports:
      - "8880:8880"

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: always
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      NVIDIA_VISIBLE_DEVICES: "all"
      NVIDIA_DRIVER_CAPABILITIES: "compute,utility"
    volumes:
      - ./ollama:/root/.ollama
    ports:
      - "11434:11434"
    stdin_open: true
    tty: true

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    restart: always
    network_mode: host
    environment:
      TTS_BACKEND_URL: http://127.0.0.1:8880
      OLLAMA_BASE_URL: http://127.0.0.1:11434
    volumes:
      - ${HOME}/open-webui:/app/backend/data
    depends_on:
      - ollama

